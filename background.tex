\subsection{Binary Instrumentation}
Recording a log of events during the execution of an application is essential for better understanding the program behavior and, in case of a failure, to locate the problem. Recording this type of information requires instrumentation of the program either at the source-code or the binary-code level. Instrumenting the source code by adding extra statements to collect the desired information is easy for developers. However, doing so modifies the code and requires recompilation, often involving multiple different tools and complex hierarchies of makefiles, which can make this approach cumbersome and frustrating for users. Instrumenting an executable at the binary level using a tool is much easier, faster, and less error prone for most users. Moreover, binary instrumentation is language independent, portable to any system that has the appropriate instrumentation tool installed, and provides machine-level insight into the behavior of the application.

Executables can be instrumented \textit{statically}, where the additional code is inserted into the binary before execution, which results in a persistent modified executable, or \textit{dynamically}, where the modification of the executable is not permanent. In dynamic binary instrumentation, code can be discovered at runtime, making it possible to handle dynamically-generated and self-modifying code. Furthermore, it may be possible to attach the instrumentation to a running process, which is particularly useful for long-running applications.

Many different tools for investigating application behavior have been designed on top of such Dynamic Binary Instrumentation (DBI) frameworks. For instance, Dyninst~\cite{dyninst} provides a dynamic instrumentation API that gives developers the ability to measure various performance aspects. It is used in tools like Open-SpeedShop~\cite{openss} and TAU~\cite{tau} as well as correctness debuggers like STAT~\cite{stat}. Moreover, VampirTrace~\cite{vampirt} uses it to provide a library for collecting program execution logs. 

Valgrind~\cite{valgrind} is a shadow-value DBI framework that keeps a copy of every register and memory location. It provides developers with the ability to instrument system calls and instructions. Error detectors such as Memcheck~\cite{memcheck} and call-graph generators like \callgrind~\cite{callgrind} are built upon Valgrind.

We designed \parlot on top of \pin~\cite{pin}, a DBI framework for the IA-32, x86-64, and MIC instruction-set architectures for creating dynamic program analysis tools. \parlot mutates \pin to track the function call stack and to trace the entry (call) and exit (return) of every executed function.


\subsection{Efficient Tracing for Debugging}
When dealing with large-scale parallel programs, any attempt to capture reasonably frequent events will result in a vast amount of data. Moreover, transferring and storing the data will incur significant overhead. For example, collecting just one byte of information per executed instruction yields on the order of a gigabyte of data per second on a single high-end core. Storing the resulting multi-gigabyte traces from many cores can be a challenge, even on today's large hard disks.

Hence, we need a way to decrease the space and runtime overhead. Compression can encode the generated data using a smaller number of bits, thus making transmitting and storing more efficient. Although the encoded data will later have to be decoded for analysis, compressing them during tracing enables the collection of much more information.

The idea of compressing traces has been used before for compressing performance traces~\cite{eventflowgraph} as well as in ScalaTrace~\cite{scalatrace}, which exploits the repetitive nature of time-step simulation in scientific applications to compress its traces~\cite{freitag}. However, only a small fraction of the compression is taking place on the fly and the focus is on reducing inter-node communication. 

In \parlot, all traced information is immediately compressed while the application is running, that is, it never even records uncompressed data in memory. As a result, typically just a few kilobytes of data need to be written per thread and per second, thus requiring only a small fraction of the available disk or network bandwidth. The traces are decompressed later at no additional cost to the execution of the application. From the decompressed full function-call trace, the complete call-graph, call-frequency, and caller-callee information can be extracted. This can be done at the granularity of a thread, a group of threads, or the whole application.

Tools like TAU~\cite{tau} and Score-p~\cite{scorep} have been using tracing and compression for performance analysis of parallel applications. STAT~\cite{stat} is one of the very few debugging tools that is based on execution tracing. After dynamically gathering function-call traces, it groups the processes with similar call stacks together and tries to find abnormal behavior like divergence in the call graph via delta debugging.

