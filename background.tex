The general idea is to utilize ParLOT \cite{ parlot} traces for studying HPC application behaviors towards fault detection and localization.
%
%ParLOT
ParLOT collects whole-program function calls and returns at different levels via dynamic binary instrumentation \cite{pin}, and incrementally compresses them on-the-fly.
%
Upon termination of the application, ParLOT flushes out per-thread trace files containing compressed sequence of executed function IDs.
%
The compression mechanism of ParLOT significantly reduces the time, memory and disk overhead, leaves the majority of the system bandwidth for the application. 
%
With the mindset of ``\textit{pay a little upfront to dramatically reduce the number of overall debug iterations}'', ParLOT well overcomes the challenge of whole-program \textit{trace collection} and leaves the \textit{trace analysis} for offline post-mortem analysis, saving HPC resources.
%

% Paragraph about DiffTrace, why we need it and what is our approach?
In this paper, we introduce DiffTrace, a tool-chain that provides an infrastructure for iterative and configurable search space reduction of the HPC whole-program function-call traces, to ease the process of detection of the most impacted trace(s) and/or region of trace(s).
%
Considering a ``successful’’ termination of the application as \textit{normal behavior}, DiffTrace takes steps towards \textit{abnormal behavior} detection when an application crashes, time outs or produces a corrupted answer.
%
Each abnormal behavior is a potential fault cause or a manifestation of the fault.
%
However, faults in HPC applications may occur or influence the program behavior at different locations and granularities, due to high and hybrid level of parallelism.
%
Also typical HPC applications spend most of their execution time in a main loop until a convergence or over timesteps, and a fault may get  triggered or causing problems after some iteration.
%
DiffTrace gives the HPC developers the capability of going through the pipeline of trace processing multiple times, each time putting a flash-light on various aspects of the applications' dynamic behavior, gradually collecting evidence about what has happened during execution.
%



\begin{itemize}
	\item \textbf{Initial Step:} Pre-processing (decompression and filter) - removing
uninteresting trace entries
	\item \textbf{Detecting loops:} Loops in source codes reflect themselves as a sequence of \textit{repetitive patterns}, resulting in often-long but redundant traces. A ``nested loop  recognition'' mechanism then mines loops from traces as ``\textit{a measure of progress}'' per thread, and also a loss-less abstraction to ease the rest of trace analysis.
%
	\item \textbf{FCA:} The control flow of events in parallel architectures often follow a \textit{regular pattern} such as SPMD, odd-even and master/slave. This characteristic often makes all traces of a single execution tend to fall into just \textit{a few} ``conceptually/behaviorally equivalent classes''. Adopted from the work by Webber et al \cite{weberStructural}, we have applied \textit{formal concept analysis} (FCA)\cite{clbook} techniques to reduce the trace search space into a few classes of traces, and also using the \textit{concept lattice} (CL) data structure as the \textit{model} of execution for further analysis.
	\item \textbf{Diffing/Ranking Suspicious Traces/Outlier detection} Measuring Buggy vs. bugFree clustering under various combination of parameters - find out which traces changed the most and rank parameters based on B-score.
	\item \textit{\textbf{gdiff:}} bolds the actual points of differences
in the outlier trace w.r.t. its corresponding normal trace - What has changed after a \textit{bug} is encountered
\end{itemize}

Odd/Even sort is a variant of the bubble-sort operates in two alternate phases: \textit{Phase-even} where even processes exchange (compare and swap) values with right neighbors and \textit{Phase-odd} where odd processes exchange values with right neighbors. Figure \ref{fig.oddEven} shows the simplified MPI implementation of the odd/even sort algorithm. We are using this example to explain our ideas and methodologies through out this section.



\begin{figure}[]
\centering
\caption{Simplified MPI implementation of Odd/Even Sort}
\includegraphics[width=0.45\textwidth]{figs/oddEven.png}
\label{fig.oddEven}
\end{figure}


\begin{figure*}[]
\caption{DiffTrace Overview}
\includegraphics[width=0.8\textwidth]{figs/diffTraceOverview.jpg}
%\includegraphics[]{figs/diffTraceOverview.jpg}
\label{fig.diffTraceOverview}
\end{figure*}


\subsection{Pre-processing}

\begin{itemize}
	\item Decompressing traces and throw away function IDs that
their function name does not match the regex of filters
	\item \textbf{filters:} There are some pre-defined filters but based on
the application semantics, one can apply any filter with
regex format.
	\item table \ref{tab:filters} shows pre-defined filters.
\end{itemize}

\input{tabs/filters.tex}


\subsection{Nested Loop Representation}

\begin{itemize}
	\item Summarizing repetitive patterns of traces in form of NLR
(Nested Loop Representation).
	\item Adopting NLR algorithm from Philip Clauss paper (explaining
the algorithm)
	\item Example on odd/even traces.
\end{itemize}

\clearpage

\subsection{Hierarchical Clustering via FCA}
\begin{itemize}
	\item Introducing FCA with some backgrounds
	\item CL incremental construction
	\item Context sample from odd/even example 
	\item Actual CL from odd/even example
	\item Obtaining JSM from reduced CL via LCA
	\item JSM sample from odd/even example
	\item Hierarchical Clustering based on the obtained JSM provides insight about general behavior of the execution 
\end{itemize}

\input{tabs/oddEvenPT.tex}

\input{tabs/tabs.sampleContext.tex}


\begin{figure}[t]
\centering
\scalebox{0.5}{
\includegraphics[width=3.4in]{figs/{sample}.pdf}}
\caption{Sample Concept Lattice from Obj-Atr Context in table\ref{tab:sampleContext}}
\label{fig:sampleCL}
\end{figure}

\begin{figure}[t]
\centering
\scalebox{0.5}{
\includegraphics[width=3.4in]{figs/{sample-reduced}.pdf}}
\caption{Concept Lattice with reduced labels}
\label{fig:sampleCL}
\end{figure}


\begin{figure}[]
\centering
\scalebox{0.8}{
\includegraphics[width=3.4in]{figs/{fancy1}.pdf}}
\caption{Pair-wise Jaccard Similarity Matrix (JSM) of MPI processes in Sample code}
\label{fig:jsm}
\end{figure}


\clearpage

\subsection{Diffing}

\begin{figure}[]
\centering
\caption{A line change in oddEvenSort (left) that might cause a deadlock in oddEvenSort\_DL (right)}
\includegraphics[width=0.45\textwidth]{figs/oddEvenDL.png}
\label{fig.oddEvenDL}
\end{figure}



\begin{itemize}
	\item Comparison of JSMs (i.e., hierarchical clusterings) would give insight about how a change (bug, library update, porting to new system) has changed the behavior of the program.
	\item Outlier detection via Diffing JSMs
	\begin{itemize}
		\item the method by Fowlkes to measure the similarity score of two clusterings
		\item detecting outlier(s) by detecting if an object (trace) falls into a different cluster 
		\item an illustration of the idea on odd/even - explaining a potential deadlock.
	\end{itemize}
	\item introducing \textit{gdiff}: bolds the actual points of differences in the outlier trace w.r.t. its corresponding normal trace
	\item sample gdiffs from odd/even example
	
\end{itemize}

\begin{figure}[]
\centering
\caption{(a) The legend of \textit{gdiff} and the list of loop structures (b) \textit{gdiff(5)} of \textit{swapBug} (c) \textit{gdiff(5)} of \textit{dlBug}}
\includegraphics[width=0.45\textwidth]{figs/sampleGdiff.png}
\label{fig.gdiffs}
\end{figure}



\subsubsection{Ranking Suspicious Traces}
\begin{itemize}
	\item Select a set of parameters and let DiffTrace generate JSMs based on the parameters (from both buggy and bug-free).
	\item Calculate B-score and outliers for all possible combinations of parameters
	\item sort outliers (i.e., suggestions as suspicious traces that suffer the most from the change) based on their B-score, lower scores shows bigger change in JSMs, higher chance of showing the actual outlier.
\end{itemize}

``Diff'' algorithm by Meyers \cite{diff-myers} takes two sequences $S_A$ and $S_B$ and computes the minimal \textit{edit} to convert $S_A$ to $S_B$. This algorithm has been used in GNU \texttt{diff} to compare two text files and in git for efficiently keeping track of file changes.
Since ParLOT preserves the order of function calls in the binary, each per thread trace $T_i$ is totally ordered, thus \textit{diff} can reflect the differences of a pair of $T$s. \textit{gdiff} is the graphical visualization of diff, aligning common and different blocks of a pair of sequences horizontally and vertically, making it easier for analyst to see the differences of a pair of sequences in a glance.  
For simplicity, our implementation of \textit{gdiff} only takes one argument $x$ as \textit{the suspicious trace}

\textit{gdiff}$(x) \equiv $ \textit{gdiff}$(T_x,T_x^\prime)$

where $T_x$ is the trace of thread/process $x$ of a normal/successful execution and $T^\prime_x$ is the corresponding trace of faulty execution.



The for loop in line 4 of \texttt{oddEvenSort()} iterates over phases of the algorithm and based on the phase, the appropriate partner for each rank is getting discovered by the function \texttt{findPtr()} (line 6). The odd/even ranks then exchange their chunks of data (lines 9-13) and a set of sort, merge and copy operations would be performed on received data by each rank (which are replaced by \texttt{...} in line 15 for simplicity).
\\

Execution of odd/even sort application with four processes (\texttt{mpirun -np 4}) while ParLOT trace collection is enabled on top of the application, would result in $T_0$, $T_1$, $T_2$ and $T_3$ (table \ref{tab:oddEvenPT}). This execution terminates successfully with expected results and the set of generated traces clearly reflects the expected behavior (control flow) of odd and even processes.
%


According to MPI Standard  \hl{[cite MPI-forum or openMPI url]}, MPI\_Send is a \textit{blocking send} used the \textit{standard} communication mode. In this mode,  MPI may buffer outgoing messages and the send call may complete before a matching receive is invoked. On the other hand, buffer space may be unavailable, or MPI may choose not to buffer outgoing messages, for performance reasons. In this case, the send call will not complete until a matching receive has been posted, and the data has been moved to the receiver. This shows that, based on the MPI implementation, the \texttt{oddEvenSort\_DL()} (figure \ref{fig.oddEvenDL}) might end up causing a deadlock, because of the order swap of MPI\_Recv and MPI\_Send in lines 11-12. 
%


%
We have planted two artificial bugs (\textit{swapBug} and \textit{dlBug}) in the code in figure \ref{fig.oddEven} and launched the code with 16 processes.
%
\textit{swapBug} swaps the order of MPI\_Send and MPI\_Recv in rank 5 after 7th iteration (of the loop in line 3 of \texttt{oddEvenSort}) simulating a potential deadlock and \textit{dlBug} simulates an actual deadlock (e.g., infinite loop) in the same location (rank 5 after 7th iteration).
%
Upon collection of ParLOT traces from execution above buggy versions, DiffTrace first decompresses traces and filters out all non-MPI functions.
Then two major loops are detected, \textbf{L0} and \textbf{L1}  (figure \ref{fig.gdiffs}-(a)) that are supposed to occur 16 times in even and odd ranks, respectively.
After analysis of CL models of execution, $x=5$ has been suggested as the most affected trace by the artificial bugs.
Figure \ref{fig.gdiffs}-(b) shows the 
\textit{gdiff}$(5)$ of \textit{swapBug} where $T_5$ iterates over the loop [MPI\_Recv - MPI\_Send] for 16 times (L1\^{}16) after the MPI initilization while the order swap has well reflected in $T_x^\prime$ (L1\^{}7 - L0\^{}9). Both processes seem to be terminated fine by executing MPI\_Finalize(). 
However, \textit{gdiff}$(5)$ of \textit{dlBug} (figure \ref{fig.gdiffs}-(c)) shows that while $T_5$ have executed MPI\_Finalize and terminated well, $T_5^\prime$ got stuck after executing L1 seven times and have never reached MPI\_Finalize.

This example shows that our approach can locate the impacted part of each execution by a fault. Having a pre-understanding of \textit{how the application should behave normally} would reduce the number of iterations by picking the right set of parameters on each pass. 





%
%In the remainder of this section, we describe a chain of techniques that we apply to collected traces with having three main goals in mind:
%\begin{itemize}
%\item selectively pre-processing of traces to prune out uninteresting trace entries when we want to study a certain aspect of the dynamic behavior of the application. 
%\item reducing the search space by forming ``equivalent classes of traces'' and locating the trace or set of traces that got impacted the most by the fault.
%\item visualizing and reflecting the impact of fault to the potential faulty trace with respect to its corresponding fault-free trace.  
%\end{itemize}