\subsection{Binary Instrumentation}
Recording a log of events during the execution of an application is essential for better understanding the program behavior and, in case of a failure, to locate the problem. Recording this type of information requires instrumentation of the program either at the source-code or the binary-code level. Instrumenting the source code by adding extra statements to collect the desired information is easy for developers. However, doing so modifies the code and requires recompilation, often involving multiple different tools and complex hierarchies of makefiles, which can make this approach cumbersome and frustrating for users. Instrumenting an executable at the binary level using a tool is much easier, faster, and less error prone for most users. Moreover, binary instrumentation is language independent, portable to any system that has the appropriate instrumentation tool installed, and provides machine-level insight into the behavior of the application.

Executables can be instrumented \textit{statically}, where the additional code is inserted into the binary before execution, which results in a persistent modified executable, or \textit{dynamically}, where the modification of the executable is not permanent. In dynamic binary instrumentation, code can be discovered at runtime, making it possible to handle dynamically-generated and self-modifying code. Furthermore, it may be feasible to attach the instrumentation to a running process, which is particularly useful for long-running applications.

Many different tools for investigating application behavior have been designed on top of such Dynamic Binary Instrumentation (DBI) frameworks. For instance, Dyninst~\cite{dyninst} provides a dynamic instrumentation API that gives developers the ability to measure various performance aspects. It is used in tools like Open-SpeedShop~\cite{openss} and TAU~\cite{tau} as well as correctness debuggers like STAT~\cite{stat}. Moreover, VampirTrace~\cite{vampirt} uses it to provide a library for collecting program execution logs. 

Valgrind~\cite{valgrind} is a shadow-value DBI framework that keeps a copy of every register and memory location. It provides developers with the ability to instrument system calls and instructions. Error detectors such as Memcheck~\cite{memcheck} and call-graph generators like \callgrind~\cite{callgrind} are built upon Valgrind.\footnote{Given the absence of tools similar to \parlot, we employ \callgrind
 as a ``close-enough'' tool in our comparisons elaborated in \S\ref{sec:tracing-tools}.
 In this capacity, \callgrind is similar to \parlotm, a variant of \parlot that only collects
 traces from the {\tt main} function. We perform such comparison to have an idea of how we fare
 with respect to one other tool. \S\ref{sec:results} separately
 presents a ``self assessment'' of \parlot.}

 
%


We designed \parlot on top of \pin~\cite{pin}, a DBI framework for the IA-32, x86-64, and MIC instruction-set architectures for creating dynamic program analysis tools.\hl{ There is also version of PIN available for ARM architecture}\cite{pinarm}. \parlot mutates \pin to track the function call stack and to trace the entry (call) and exit (return) of every executed function. \hl{However, the mechanism of our tracing and compressing approaches can be built on top of other instrumentation tools and not restricted to pin. For example, PMaC }\cite{pmac} \hl{ is a DBI tool available for PowerPC/AIX architecture and ParLOT can be built on top of that.}


\subsection{Efficient Tracing for Debugging}
When dealing with large-scale parallel programs, any attempt to capture reasonably frequent events will result in a vast amount of data. Moreover, transferring and storing the data will incur significant overhead. For example, collecting just one byte of information per executed instruction yields on the order of a gigabyte of data per second on a single high-end core. Storing the resulting multi-gigabyte traces from many cores can be a challenge, even on today's large hard disks.

Hence, we need a way to decrease the space and runtime overhead. Compression can encode the generated data using a smaller number of bits, help
reduce the amount of data movement across the memory hierarchy, and
reduce storage and network demands.
%
Although the encoded data will later have to be decoded for analysis, compressing them during tracing enables the collection of much more information.

The use of compression by itself is not new.
Various performance evaluation tools~\cite{tau,scorep,eventflowgraph} 
already employ compression during the collection
of performance analysis data.
%
Tools such as ScalaTrace~\cite{scalatrace}
are also known to exploit
the repetitive nature of time-step simulations~\cite{freitag}. %Martin: unclear to me

\hl{
Many performance and debugging tools for HPC applications} \cite{stat,taumrnet}\hl{ had been built on top of MRNet}\cite{mrnet}\hl{ as an efficient overlay network for customizable data collection. MRNet overcomes the challenge of managing massive amount of trace data by distributing the workload of data collection, transfer and analysis among processes on a tree-based network. In contrast, ParLOT is taking advantage of data compression to increase efficiency and vastly reduce required bandwidth for data transfer. }

The novelty offered by \parlot lies in the combination of compression
speed, efficacy, and low timing jitter
made possible by its {\em incremental}
compression algorithm, which is
described in \S\ref{sec:design}.
%
It immediately compresses all traced information while the application is running, that is, \parlot never even records the uncompressed trace in memory. 
%
Typically, just a few kilobytes of data need to be written out per thread and per second, thus requiring only a small fraction of the available disk or network bandwidth. 
%
The traces are decompressed later at no additional cost to the execution of the application. 
%
From the decompressed full function-call trace, the complete call-graph, 
call-frequency, and caller-callee information can be extracted. 
%
This can be done at the granularity of a thread, a group of threads, or the whole application.
%
We now elaborate on the design of \parlot that makes
these innovations possible.
%--







