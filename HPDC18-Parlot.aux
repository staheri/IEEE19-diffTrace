\relax 
\newlabel{abs}{{}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{sec:intro}{{I}{1}}
\citation{dyninst}
\citation{openss}
\citation{tau}
\citation{stat}
\citation{vampirt}
\citation{stat}
\citation{valgrind}
\citation{memcheck}
\citation{callgrind}
\citation{ipm}
\citation{tau}
\citation{scorep}
\citation{vtune}
\citation{event-flow-graph}
\citation{scalatrace}
\citation{freitag}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background}{2}}
\newlabel{sec:background}{{II}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}PIN}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Compression}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Related Tools}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Implementation}{2}}
\newlabel{sec:impl}{{IV}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Tracing Operation}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Call-Stack Correction}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Incremental Compression}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}Compression Algorithm}{3}}
\bibstyle{IEEEtran}
\bibdata{bibs}
\bibcite{dyninst}{1}
\bibcite{openss}{2}
\bibcite{tau}{3}
\bibcite{stat}{4}
\bibcite{vampirt}{5}
\bibcite{valgrind}{6}
\bibcite{callgrind}{7}
\bibcite{ipm}{8}
\bibcite{event-flow-graph}{9}
\bibcite{scalatrace}{10}
\bibcite{freitag}{11}
\@writefile{toc}{\contentsline {section}{\numberline {V}Results}{4}}
\newlabel{sec:res}{{V}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Summary, Conclusion and Future Work}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-A}}Summary and Conclusion}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-B}}Future Work}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Appendix}{4}}
\@writefile{toc}{\contentsline {section}{References}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Server: \textbf  {PSC} - This table contains Slowdowns of ParLOT and Callgrind (slowdowns are relative to pure run). The input size is \textbf  {B}. NAS benchmark input sizes are as follows : $size(A) < size(B) < size (C) < size(D) $. In later tables and charts I show that ParLOT has better performance on larger inputs (like C and D). I was not able to run Callgrind with input size of C and D since it was time consuming, crashing and wasting SUs. Also I only included the results for up to 16 nodes (256 cores) in this table. Almost all of the experiments with Callgrind on 64 nodes (1024 cores) crashed [I documented all sort of crashing reasons of Callgrind on 1024 cores]. ParLOT results of 64 nodes will appear in later tables. I grouped the results of experiments with similar input sizes and nodes (group of 3 rows). Each row is in this format \textbf  {Tool.Input.Nodes}. Last column of the table (GM) is GeoMean of all values in that row. By comparing the values of GM row, we can see that ParLOT(both main and all) has better performance comparing to Callgrind. However, it seems that Callgrind scales better (more about this in next table). ( Fig 1\hbox {}). Format of next two tables are similar to this one but numbers are from Comet and Lonestar, respectively(tables II\hbox {} and III\hbox {})}}{5}}
\newlabel{sd_pMpAcg_B_int_p3.5}{{I}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Server: \textbf  {Comet} - Stat: \textbf  {Detail Slowdown} - Tools: pinMain , pinAll , callgrind - Inputs: B - Nodes: 1 , 4 , 16 , 64 - Desc: Primary - Similar to table I\hbox {} but numbers are from Comet}}{5}}
\newlabel{comet_sd_pMpAcg_B_int_p3.5}{{II}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Server = \textbf  {Lonestar5} - Stat: \textbf  {Slowdown} - Tools: pinMain , pinAll , callgrind - Inputs: B - Nodes: 1 , 4 , 16 - Desc: Primary - (similar to table I\hbox {} but numbers are from Lonestar5) Callgrind failed on Lonestar5. I have this table for comparison of ParLOT between different servers}}{5}}
\newlabel{ls5_sd_pMpAcg_B_int_p3.5}{{III}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Server: \textbf  {PSC} - The values in this table is identical to table I\hbox {} but grouped differently to show the scalability of each tool. The slowdown of Callgrind drops drastically with increasing cores from 16 to 64. For ParLOT (for both main and all), slowdowns are higher for larger number of cores. However, the average GeoMean of all slowdowns (numbers in bold) show that ParLOT has better overall performance. The \textbf  {AVG} rows contain the average of its above 3 values.( Fig 1\hbox {}) - Format of next two tables are similar to this one but numbers are from Comet and Lonestar, respectively(tables V\hbox {} and VI\hbox {})}}{6}}
\newlabel{sd_pMpAcg_B_itn_p3.5}{{IV}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Server: \textbf  {Comet} - Stat: \textbf  {Detail Slowdown} - Tools: pinMain , pinAll , callgrind - Inputs: B - Nodes: 1 , 4 , 16 , 64 - Desc: Primary - Similar to table IV\hbox {} but numbers are from Comet}}{6}}
\newlabel{comet_sd_pMpAcg_B_itn_p3.5}{{V}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Server = \textbf  {Lonestar5} - Stat: \textbf  {Slowdown} - Tools: pinMain , pinAll , callgrind , Inputs: B , Nodes: 1 , 4 , 16 , Desc: Primary - Callgrind failed on Lonestar5. I have this table for comparison of ParLOT between different servers}}{6}}
\newlabel{ls5_sd_pMpAcg_B_itn_p3.5}{{VI}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Server: \textbf  {PSC} - This table is showing the required bandwidth for each application (KiloBytes per core per second). $ReqBW_x = TraceSize_x (KB) / (\# of cores)_x / Runtime_x (S)$ Because of the crashing problems of Callgrind on \textit  {C} input and 64 nodes, I only include \textit  {B} input and 1, 4, and 16 nodes results. Clearly ParLOT(main) is beating Callgrind while they both generate the same information (I still believe ParLOT(main) generated traces are more informative and rich). ParLOT(all) bandwidth is the highest but with capturing all of the function calls within a single execution, there is no surprise. Another interesting fact from this table is, for ParLOT(main), bandwidth drops from \textbf  {0.62} for 16 cores to \textbf  {0.27} for 256 cores (good scalability). It is the opposite for Callgrind where the required bandwidth jumps from \textbf  {3.28} (KB/s) for 16 cores to \textbf  {33.06} (KB/S) for 256 cores. I also have the results of required bandwidth of ParLOT for 64 nodes(1024 cores) and Input C but I did not include them here because I did not have them for Callgrind (explained above). Fig 3\hbox {} visualize these numbers (Average values) - Format of next two tables are similar to this one but numbers are from Comet and Lonestar, respectively(tables VIII\hbox {} and IX\hbox {})}}{7}}
\newlabel{bw_pMpAcg_B_itn_p3.5}{{VII}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces Server: \textbf  {Comet} - Stat: \textbf  {Bandwidth} - Tools: pinMain , pinAll , callgrind - Inputs: B - Nodes: 1 , 4 , 16 , 64 - Desc: Primary}}{7}}
\newlabel{comet_bw_pMpAcg_B_itn_p3.5}{{VIII}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {IX}{\ignorespaces Server = \textbf  {Lonestar5} - Stat: \textbf  {Bandwidth} - Tools: pinMain , pinAll , callgrind - Inputs: B - Nodes: 1 , 4 , 16 - Desc: Primary}}{7}}
\newlabel{ls5_bw_pMpAcg_B_itn_p3.5}{{IX}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {X}{\ignorespaces Server: \textbf  {PSC} - \textbf  {COMPRESSION RATIOS} FOR B AND C INPUTS AND 1, 4, 16 AND 64 NODES.}}{8}}
\newlabel{cr_pMpA_BC_itn_p3.5}{{X}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {XI}{\ignorespaces Server: \textbf  {Comet} - \textbf  {COMPRESSION RATIOS} FOR B INPUTS AND 1, 4, 16 AND 64 NODES - C input experiments are cooking}}{8}}
\newlabel{comet_cr_pMpA_B_itn_p3.5}{{XI}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {XII}{\ignorespaces Server: \textbf  {PSC} - Tables XII\hbox {}, XIII\hbox {}, XIV\hbox {}, XV\hbox {}, XVI\hbox {} and XVII\hbox {} are showing the detail slowdowns added to the code by each phase of ParLOT(main and all) for input size \textit  {B} on \textbf  {PSC}, \textbf  {Comet} and \textbf  {Lonestar5}. I put my observations of all four tables over here. \textbf  {npin} is just the slowdown caused by initializing Pin's routines on top of the target application without doing anything else (no instrumentation, tracing, compression and I/O. \textbf  {dpin} is almost identical to ParLOT except it stores the generated compressed traces to "/dev/null". The purpose of \textit  {dpin} is to see how much of the overall overhead is because of I/O and data-related slowdowns. In \textbf  {wpin}, and all collected data would be stored as is to the disk. The results of this tools shows how much efficiency our compression approach adds to ParLOT. Last row of tables shows geometric mean of each of its above values showing how much each phase of ParLOT slows down the native execution. In general, we all expect that the slowdowns of $npin < dpin < ParLOT < wpin $. But majority of numbers are not like that. I double checked the results of CHPC and Stampede and the patterns are kind of identical. In most of the table entries (in particular for smaller number of cores), the differences between the average slowdown of \textit  {dpin} and \textit  {ParLOT} is very insignificant which shows that ParLOT is not an I/O-bounded tool. Figures 6\hbox {} and 7\hbox {} visualize numbers from these tables.}}{8}}
\newlabel{det_Main_all_B_p3.5}{{XII}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {XIII}{\ignorespaces Server: \textbf  {Comet} - Stat: \textbf  {Detail Slowdown} - Inputs: B - Nodes: 1 , 4 , 16 , 64 - Desc: Detail Report}}{9}}
\newlabel{comet_det_Main_all_B_p3.5}{{XIII}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {XIV}{\ignorespaces Server = \textbf  {Lonestar5} - Stat: \textbf  {Detail Slowdown} - Inputs: B - Nodes: 1 , 4 , 16 , 64 - Desc: Detail Report}}{9}}
\newlabel{ls5_det_Main_all_B_p3.5}{{XIV}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {XV}{\ignorespaces server: \textbf  {PSC} - \textbf  {Detail Slowdown} - Inputs: B - Nodes: 1 , 4 , 16 , 64 - Desc: Detail Report}}{9}}
\newlabel{det_All_all_B_p3.5}{{XV}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {XVI}{\ignorespaces Server: \textbf  {Comet} - Stat: \textbf  {Detail Slowdown} - Inputs: B - Nodes: 1 , 4 , 16 , 64 - Desc: Detail Report}}{9}}
\newlabel{comet_det_All_all_B_p3.5}{{XVI}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Server: \textbf  {PSC} - Slowdown of ParLOT(main,all) and Callgrind. Each bar is the average slowdown of each tool on each application for 1, 4 and 16 nodes (16, 64 and 256 cores). Last group of bars is GeoMean (from bold numbers in table IV\hbox {}) Figures ?????????????. }}{10}}
\newlabel{chartAvg_sd_B_p3_5}{{1}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Server: \textbf  {Comet} - Slowdown of ParLOT(main,all) and Callgrind. (similar to fig 1\hbox {}) }}{10}}
\newlabel{comet_chartAvg_sd_B_p3_5}{{2}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {XVII}{\ignorespaces Server = \textbf  {Lonestar5} - Stat: \textbf  {Detail Slowdown} - Inputs: B - Nodes: 1 , 4 , 16 , 64 - Desc: Detail Report}}{10}}
\newlabel{ls5_det_All_all_B_p3.5}{{XVII}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Server: \textbf  {PSC} - Required Bandwidth (KB) per core per second for ParLOT and Callgrind. (Input: B)}}{11}}
\newlabel{chartAvg_bw_B_p3_5}{{3}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Server: \textbf  {Comet} - Required Bandwidth (KB) per core per second for ParLOT and Callgrind. (Input: B)}}{11}}
\newlabel{comet_chartAvg_bw_B_p3_5}{{4}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {XVIII}{\ignorespaces Server: \textbf  {Comet} - Stat: \textbf  {Detail Slowdown including apin(fwrite(,,0,))} - Inputs: B - Nodes: 1 , 4 , 16 , 64 - Desc: Detail Report 2}}{11}}
\newlabel{comet_det_Main_all_B2_p3.5}{{XVIII}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Server: \textbf  {PSC} - Input size: \textbf  {B}. Each bar is stacked value of slowdowns : $Native Run = 1$ , $Pure Pin = npin - 1$ , $Tracing = ParLOT - npin$. Label of each bar is, (main/all).(1/4/16/64). This graph shows why ParLOT does not scale that well. The overhead that Pin itself is adding to the native run is growing with higher number of cores. The green part of each bar (tracing) is the overhead that our approach is adding. Fig 7\hbox {} shows the effectiveness of our compression approach}}{12}}
\newlabel{chartDet_B_wc_byTool_p3_5}{{5}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Server: \textbf  {Comet} - Input size: \textbf  {B}. Similar to fig 6\hbox {}}}{12}}
\newlabel{chartDet_B_wc_byTool_p3_5}{{6}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Server: \textbf  {PSC} - Input size: \textbf  {B}. Each bar is stacked value of slowdowns : $Native Run = 1$ , $Pure Pin = npin - 1$ , $Tracing (w/o \_compression) = wpin - npin$. This graph clearly shows how much impact our compression method has on the performance of ParLOT. }}{13}}
\newlabel{chartDet_B_woc_byTool_p3_5}{{7}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Server: \textbf  {Comet} - Input size: \textbf  {B}. Each bar is stacked value of slowdowns : $Native Run = 1$ , $Pure Pin = npin - 1$ , $Tracing (w/o \_compression) = wpin - npin$. This graph clearly shows how much impact our compression method has on the performance of ParLOT. }}{13}}
\newlabel{comet_chartDet_B_woc_byTool_p3_5}{{8}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {XIX}{\ignorespaces Server: \textbf  {Comet} - Stat: \textbf  {Detail Slowdowns including apin(fwrite(,,0,))} - Inputs: B - Nodes: 1 , 4 , 16 , 64 - Desc: Detail Report 2}}{14}}
\newlabel{comet_det_All_all_B2_p3.5}{{XIX}{14}}
