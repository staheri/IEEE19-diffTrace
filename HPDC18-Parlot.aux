\relax 
\newlabel{abs}{{}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{sec:intro}{{I}{1}}
\citation{dyninst}
\citation{openss}
\citation{tau}
\citation{stat}
\citation{vampirt}
\citation{stat}
\citation{valgrind}
\citation{memcheck}
\citation{callgrind}
\citation{ipm}
\citation{tau}
\citation{scorep}
\citation{vtune}
\citation{event-flow-graph}
\citation{scalatrace}
\citation{freitag}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background}{2}}
\newlabel{sec:background}{{II}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}PIN}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Compression}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Related Tools}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Implementation}{2}}
\newlabel{sec:impl}{{IV}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Tracing Operation}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Call-Stack Correction}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Incremental Compression}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}Compression Algorithm}{3}}
\bibstyle{IEEEtran}
\bibdata{bibs}
\bibcite{dyninst}{1}
\bibcite{openss}{2}
\bibcite{tau}{3}
\bibcite{stat}{4}
\bibcite{vampirt}{5}
\bibcite{valgrind}{6}
\bibcite{callgrind}{7}
\bibcite{ipm}{8}
\bibcite{event-flow-graph}{9}
\bibcite{scalatrace}{10}
\bibcite{freitag}{11}
\@writefile{toc}{\contentsline {section}{\numberline {V}Results}{4}}
\newlabel{sec:res}{{V}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Summary, Conclusion and Future Work}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-A}}Summary and Conclusion}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {VI-B}}Future Work}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Appendix}{4}}
\@writefile{toc}{\contentsline {section}{References}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces This table contains Slowdowns of ParLOT and Callgrind (slowdowns are relative to pure run). The input size is \textbf  {B}. NAS benchmark input sizes are as follows : $size(A) < size(B) < size (C) < size(D) $. In later tables and charts I show that ParLOT has better performance on larger inputs (like C and D). I was not able to run Callgrind with input size of C and D since it was time consuming, crashing and wasting SUs. Also I only included the results for up to 16 nodes (256 cores) in this table. Almost all of the experiments with Callgrind on 64 nodes (1024 cores) crashed [I documented all sort of crashing reasons of Callgrind on 1024 cores]. ParLOT results of 64 nodes will appear in later tables. I grouped the results of experiments with similar input sizes and nodes (group of 3 rows). Each row is in this format \textbf  {Tool.Input.Nodes}. Last column of the table (GM) is GeoMean of all values in that row. By comparing the values of GM row, we can see that ParLOT(both main and all) has better performance comparing to Callgrind. However, it seems that Callgrind scales better (more about this in next table). ( Fig 1\hbox {})}}{5}}
\newlabel{sd_pMpAcg_B_int_p3.5}{{I}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Stat: sd Tools: pinMain , pinAll , callgrind , Inputs: B , Nodes: 1 , 4 , 16 , Desc: Primary}}{5}}
\newlabel{ls5_sd_pMpAcg_B_int_p3.5}{{II}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces The values in this table is identical to table I\hbox {} but grouped differently to show the scalability of each tool. The slowdown of Callgrind drops drastically with increasing cores from 16 to 64. For ParLOT (for both main and all), slowdowns are higher for larger number of cores. However, the average GeoMean of all slowdowns (numbers in bold) show that ParLOT has better overall performance. The \textbf  {AVG} rows contain the average of its above 3 values.( Fig 1\hbox {})}}{5}}
\newlabel{sd_pMpAcg_B_itn_p3.5}{{III}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Slowdown of ParLOT(main,all) and Callgrind. Each bar is the average slowdown of each tool on each application for 1, 4 and 16 nodes (16, 64 and 256 cores). Last group of bars is GeoMean (from bold numbers in table III\hbox {}). }}{6}}
\newlabel{chartAvg_sd_B_p3_5}{{1}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Stat: sd Tools: pinMain , pinAll , callgrind , Inputs: B , Nodes: 1 , 4 , 16 , Desc: Primary}}{6}}
\newlabel{ls5_sd_pMpAcg_B_itn_p3.5}{{IV}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces The purpose of this table is to show that ParLOT is doing better job on larger input sizes. NAS benchmark input sizes are as follows : $size(A) < size(B) < size (C) < size(D) $. The overhead it adds to the application is smaller for input size C and I believe the reason is the redundant captured data (function calls) for each run helps the performance of compressing process, thus helps the overall performance. I can run these experiments with smaller input size (A) or larger (D) and include them in this table. Running NAS applications with \textit  {A} makes running times so small (less than a second for some of applications) and running with \textit  {D} is going to consume a lot of SUs, if we decide to do that. Also these are the results when I use the latest version of \textit  {Pin} which is \textbf  {3.5}. Table VI\hbox {} (next one) is identical to this table but using version \textbf  {3.0} of \textit  {Pin}}}{7}}
\newlabel{sd_pMpA_BC_tni_p3.5}{{V}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces This table is similar to previous one (table V\hbox {}) but with version \textbf  {3.0} of Pin. There are some zeros in the table which shows those experiments have crashed (probably because of incompatiblity of Pin with current configuration on PSC nodes). Also some of the slowdowns are smaller than 1. Thus the values of this table are not accurate and probably invalid (at least for larger number of cores). But I just wanted to include them here, in the first draft of paper, to show some differences between Pin versions and in case you are wondering why the numbers that I had in October was much better than these numbers (tables that came previously), the reason is Pin versions.}}{7}}
\newlabel{sd_pMpA_BC_tni_p3.0}{{VI}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Stat: sd Tools: pinMain , pinAll , Inputs: B , C , Nodes: 1 , 4 , 16 , 64 , Desc: Primary}}{8}}
\newlabel{ls5_sd_pMpA_BC_tni_p3.5}{{VII}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces This table is showing the required bandwidth for each application (KiloBytes per core per second). $ReqBW_x = TraceSize_x (KB) / (\# of cores)_x / Runtime_x (S)$ Because of the crashing problems of Callgrind on \textit  {C} input and 64 nodes, I only include \textit  {B} input and 1, 4, and 16 nodes results. Clearly ParLOT(main) is beating Callgrind while they both generate the same information (I still believe ParLOT(main) generated traces are more informative and rich). ParLOT(all) bandwidth is the highest but with capturing all of the function calls within a single execution, there is no surprise. Another interesting fact from this table is, for ParLOT(main), bandwidth drops from \textbf  {0.62} for 16 cores to \textbf  {0.27} for 256 cores (good scalability). It is the opposite for Callgrind where the required bandwidth jumps from \textbf  {3.28} (KB/s) for 16 cores to \textbf  {33.06} (KB/S) for 256 cores. I also have the results of required bandwidth of ParLOT for 64 nodes(1024 cores) and Input C but I did not include them here because I did not have them for Callgrind (explained above). Fig 2\hbox {} visualize these numbers (Average values)}}{8}}
\newlabel{bw_pMpAcg_B_itn_p3.5}{{VIII}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Required Bandwidth (KB) per core per second for ParLOT and Callgrind. (Input: B)}}{9}}
\newlabel{chartAvg_bw_B_p3_5}{{2}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {IX}{\ignorespaces Similar to previous table (table VIII\hbox {}) for all number of nodes for \textbf  {C} input}}{9}}
\newlabel{bw_pMpA_C_itn_p3.5}{{IX}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {X}{\ignorespaces COMPRESSION RATIOS FOR B AND C INPUTS AND 1, 4, 16 AND 64 NODES.}}{9}}
\newlabel{cr_pMpA_BC_itn_p3.5}{{X}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {XI}{\ignorespaces Table XI\hbox {}, XIII\hbox {}, XV\hbox {} and XVII\hbox {} are showing the detail slowdowns added to the code by each phase of ParLOT(main and all) for input sizes of \textit  {B} and \textit  {C}. I put my observations of all four tables over here. \textbf  {npin} is just the slowdown caused by initializing Pin's routines on top of the target application without doing anything else (no instrumentation, tracing, compression and I/O. \textbf  {dpin} is almost identical to ParLOT except it stores the generated compressed traces to "/dev/null". The purpose of \textit  {dpin} is to see how much of the overall overhead is because of I/O and data-related slowdowns. In \textbf  {wpin}, and all collected data would be stored as is to the disk. The results of this tools shows how much efficiency our compression approach adds to ParLOT. Last row of tables shows geometric mean of each of its above values showing how much each phase of ParLOT slows down the native execution. In general, we all expect that the slowdowns of $npin < dpin < ParLOT < wpin $. But majority of numbers are not like that. For larger input sizes (tables XVII\hbox {} and XIII\hbox {}) and also for for ParLOT(all) (tables XV\hbox {} and XVII\hbox {}) the GeoMean row numbers make more sense. I double checked the results of CHPC and Stampede and the patterns are kind of identical. In most of the table entries (in particular for smaller number of cores), the differences between the average slowdown of \textit  {dpin} and \textit  {ParLOT} is very insignificant which shows that ParLOT is not an I/O-bounded tool. One of the problems that I had and still have is running \textit  {wpin} on input \textit  {C}. I did not have the results of \textit  {wpin} on \textit  {C} input. While I was preparing these tables and I felt secure about not wasting SUs, I let \textit  {wpin} run on \textit  {C} input. Unfortunately, most of the experiments crashed due to a \textit  {PMPI} error so the results are unreliable. That is why I put 0 in table XIII\hbox {} and XVII\hbox {} under \textit  {wpin}. Figures 3\hbox {}, 4\hbox {} and 5\hbox {} visualize numbers from these tables.}}{10}}
\newlabel{det_Main_all_B_p3.5}{{XI}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Input size: \textbf  {B}. Each bar is stacked value of slowdowns : $Native Run = 1$ , $Pure Pin = npin - 1$ , $Tracing = ParLOT - npin$. Label of each bar is, (main/all).(1/4/16/64). This graph shows why ParLOT does not scale that well. The overhead that Pin itself is adding to the native run is growing with higher number of cores. The green part of each bar (tracing) is the overhead that our approach is adding. Fig 4\hbox {} shows the effectiveness of our compression approach}}{10}}
\newlabel{chartDet_B_wc_byTool_p3_5}{{3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Input size: \textbf  {B}. Each bar is stacked value of slowdowns : $Native Run = 1$ , $Pure Pin = npin - 1$ , $Tracing (w/o \_compression) = wpin - npin$. This graph clearly shows how much impact our compression method has on the performance of ParLOT. }}{11}}
\newlabel{chartDet_B_woc_byTool_p3_5}{{4}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Input size: \textbf  {C}. This chart is similar to fig 3\hbox {} but for larger input size C. As I mentioned in the table description, ParLOT seem to have better performance on larger input sizes. General shape of this chart matches fig 3\hbox {} which shows the deterministic behavior of our tool and application.}}{11}}
\newlabel{chartDet_C_wc_byTool_p3_5}{{5}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {XII}{\ignorespaces Stat: sd Tools: Inputs: B , Nodes: 1 , 4 , 16 , 64 , Desc: Detail Report}}{12}}
\newlabel{ls5_det_Main_all_B_p3.5}{{XII}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {XIII}{\ignorespaces Input: C , Main}}{12}}
\newlabel{det_Main_all_C_p3.5}{{XIII}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {XIV}{\ignorespaces Stat: sd Tools: Inputs: C , Nodes: 1 , 4 , 16 , 64 , Desc: Detail Report}}{12}}
\newlabel{ls5_det_Main_all_C_p3.5}{{XIV}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {XV}{\ignorespaces Input: B , All}}{12}}
\newlabel{det_All_all_B_p3.5}{{XV}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {XVI}{\ignorespaces Stat: sd Tools: Inputs: B , Nodes: 1 , 4 , 16 , 64 , Desc: Detail Report}}{13}}
\newlabel{ls5_det_All_all_B_p3.5}{{XVI}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {XVII}{\ignorespaces  Input: C , All}}{13}}
\newlabel{det_All_all_C_p3.5}{{XVII}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {XVIII}{\ignorespaces Stat: sd Tools: Inputs: C , Nodes: 1 , 4 , 16 , 64 , Desc: Detail Report}}{13}}
\newlabel{ls5_det_All_all_C_p3.5}{{XVIII}{13}}
