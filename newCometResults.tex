%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tracing Overhead
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Tracing Overhead}
\label{subsec:lowtoh}
\input{tabs.comet.newMed/comet_bw_pMpAcg_BC_itn_p3.5.tex}

Table \ref{comet_sd_pMpAcg_BC_itn_p3.5} shows the tracing overhead of \parlotm, \parlota, and \callgrind on each application of the NPB benchmark suite for different node counts. The last column of the table lists the geometric mean over all eight programs. The AVG rows show the average over the four node counts.


On average, both \parlotm and \parlota outperform \callgrind. The bolded numbers in Table \ref{comet_sd_pMpAcg_BC_itn_p3.5} for input C show that the average overhead is 1.94 for \parlotm, 2.73 for \parlota, and 4.63 for \callgrind. Figures \ref{comet_chartAvg_sd_B_p3_5} and \ref{comet_chartAvg_sd_C_p3_5} show these results in visual form.


Perhaps the key takeaway point is that the overhead of \parlot is roughly a factor of two to three, which we believe users are willing to accept, e.g. if it helps them debug their applications. This is very promising especially when considering how detailed the collected trace information is. Similarly, it is reassuring to see that \parlot 's overhead is typically lower than that of \callgrind, which collects less information.

The overhead of \parlot increases as we scale the applications to more compute nodes. However, the increase is quite small. Going from 16 to 1024 cores, a 64-fold increase in parallelism, only increases the average overhead by between 1.3- and 2.1-fold. In contrast, \callgrind 's overhead decreases with increasing node count, making it more scalable. Having said that, \callgrind 's overhead is larger for the C inputs whereas \parlot 's overhead is larger for the smaller B inputs. In other words, \parlot scales better to larger inputs than \callgrind.



\parlot 's scaling behavior can be explained by correlating it with the expected function-call frequency. When distributing a fixed problem size over more cores, each core receives less work. As a consequence, less time is spent in the functions that process the work, resulting in more function calls per time unit, which causes more work for \parlot. In contrast, when distributing a larger problem size over the same number of cores, each core receives more work. Hence, more time is spent in the functions that process the work, resulting in fewer function calls per time unit, which causes less work for \parlot and therefore less tracing overhead. Hence, we believe \parlot 's overhead to be even lower on long-running inputs, which is where our tracing technique is needed the most.


In summary, \parlot 's overhead is in the single digits for all evaluated applications and configurations, including for 1024-core runs. It appears to scale reasonably to larger node counts and well to larger problem sizes.\hl{ Also, results in} \S\ref{subsec:lowbw} show that average amount of trace data collected by \parlota is almost 360 times larger than \callgrind while the overhead \parlota adds is 60\% of added overhead by \callgrind.

  
\subsection{Required Bandwidth}
\label{subsec:lowbw}



Table \ref{comet_bw_pMpAcg_BC_itn_p3.5}, Fig.  \ref{comet_chartAvg_bw_B_p3_5} and Fig.\ref{comet_chartAvg_bw_C_p3_5} show how much bandwidth each tool 
requires
during the application execution. 
%
We notice that \parlotm requires much less bandwidth than
\callgrind, especially for smaller inputs. 
%
\parlota's bandwidth is much higher as it collects call info from all
images, and not just the main image.


We observe from Table \ref{comet_cr_pMpA_BC_itn_p3.5}, input C, that the
average compression ratio for \parlota is 644.38, and its
corresponding required bandwidth from Table
\ref{comet_bw_pMpAcg_BC_itn_p3.5} is 56.38.
%
This means \parlot can
collect \textbf{more than 36 MB} worth of data per core per second
while only needing 56 kB/s of the system bandwidth, {\em leaving the rest of the available bandwidth for application.}
%
In comparison, \callgrind only
collects \textbf{less than 100 kB} of data but still adds more
overhead compared to either \parlota or \parlotm . 
%
The average amount of trace data that can be collected by \parlota is
\textbf{360x} (85x for \parlotm) larger than that for \callgrind.
%
\hl{We see the required bandwidth for different input sizes of NPB applications are almost equal in }\parlot and drop by a factor of 2 in \callgrind when doubling the input size from B to C. According to NPB documentation, number of iterations in both input B and C are the same for all applications and their difference is the number of elements or the grid size. It is clear that the required bandwidth of \parlot is independent of the problem size, unlike \callgrind\hl{ where input size has linear impact on results.}
%








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\subsection{Compression Ratio}
\label{subsec:cr}

Table \ref{comet_cr_pMpA_BC_itn_p3.5} shows the compression ratios for all configurations and inputs. 
On average, \parlot can store up to more than 1.7 GB of collected data in a 1 MB trace. 
In fact, the compression mechanism of
\parlot can achieve up to 21k compression ratio.
%
Furthermore, this result was achieved with low overhead and incremental  
on-the-fly compression.
%
Generally, the obtained
compression ratios for \parlotm are higher than for \parlota. Also by looking at Fig. \ref{comet_chartAvg_bw_B_p3_5}, Fig. \ref{comet_chartAvg_bw_C_p3_5}, Fig. \ref{comet_chartAvg_cr_B_p3_5} and Fig. \ref{comet_chartAvg_cr_C_p3_5},\hl{ we can see EP with maximum compression ratio among other NPB applications has the minimum required bandwidth; the story is the opposite way for CG which has minimum compression ratio and maximum required bandwidth. CG is a conjugate gradient method with irregular memory accesses and communications and EP is an "embarrassingly parallel" random number generator. The main difference between these two applications is the amount of interprocessor communication involved during execution where EP has no significant communication and CG tests irregular long distance communication employing  unstructured matrix vector multiplication. (Saeed After I wrote , this I noticed that our tool has best performance on the EMBARRASSINGLY PARALLEL application with almost no interprocessor communication. Maybe it is better to omit EP discussion and only talk about CG here. thoughts? ) }
%

\parlot's \hl{compression mechanism works better on larger input sizes. (Saeed: I am not sure exactly about the explanation of this)}
  
\input{tabs.comet.newMed/comet_cr_pMpA_BC_itn_p3.5.tex}

\begin{figure}[t]
\centering
\includegraphics[width=3.8in]{figs.comet.newMed/comet_chartDet_B_wc_byTool_p3_5.png}
\caption{ Tracing overhead added to the native run by each tool - Input: B}
\label{comet_chartDet_B_wc_byTool_p3_5}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=3.8in]{figs.comet.newMed/comet_chartDet_C_wc_byTool_p3_5.png}
\caption{ Tracing overhead added to the native run by each tool - Input: C}
\label{comet_chartDet_C_wc_byTool_p3_5}
\end{figure}




  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
  
\subsection{ \pininit Overhead} 
\label{subsec:pinit}
Tables \ref{comet_wo_det_Main_all_B_p3.5} and
\ref{comet_wo_det_All_all_B_p3.5} present the average overhead added to each
application by different versions of \parlot. 
%
The last row of these two tables
presents the geometric mean.

%
This information captures how much each
phase of \parlot slows down the native execution. 

\input{tabs.comet.newMed/comet_wo_det_Main_all_B_p3.5.tex}

\input{tabs.comet.newMed/comet_wo_det_All_all_B_p3.5.tex}

In general, one 
expects the following inequality to hold:
 the added overhead of \pininit < that of \parlot
< that of \parlotnc. 
%
The highlighted cells in these tables do not
obey this inequality.
%
In fact, the variability across three runs of each experiment
is shown in Fig. \ref{comet_BX2_Main_16_B_p3_5}
where we present the minimum, maximum and median overheads.
%
These
overheads are for input size B and 16 nodes. 
%
This variability explains the seeming inconsistencies in  Tables
\ref{comet_wo_det_Main_all_B_p3.5} and
\ref{comet_wo_det_All_all_B_p3.5}.


Tables \ref{comet_wo_det_Main_all_B_p3.5} and
\ref{comet_wo_det_All_all_B_p3.5} show that, on average, \pininit adds
an overhead of 3.28  and \parlota adds an overhead of 3.42. 
%
\textbf{Almost 96\%
of \parlota overhead is due to \pin}. 
%
The results of \parlotm and
other inputs are follow the same pattern
(see fig. \ref{comet_chartDet_B_wc_byTool_p3_5} and \ref{comet_chartDet_C_wc_byTool_p3_5}). 
%
The overhead that \parlot (excluding the overhead of \pininit) {\em adds}
to applications is very small.
%
We further observe that if we were to switch to a different
instrumentation tool that is not as general as \pin but perhaps more
lightweight, the overall overhead would potentially reduce drastically. \\


\begin{figure}[t]
\centering
\includegraphics[width=3.8in]{figs.comet.newMed/comet_chartDet_B_woc_byTool_p3_5.png}
\caption{Tracing overhead added by \pininit and \parlotnc - Input: B 
}
\label{comet_chartDet_B_woc_byTool_p3_5}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=3.8in]{figs.comet.newMed/comet_chartDet_C_woc_byTool_p3_5.png}
\caption{Tracing overhead added by \pininit and \parlotnc -  Input: C}
\label{comet_chartDet_C_woc_byTool_p3_5}
\end{figure}

\subsection{Compression Impact} 
\label{subsec:compact}

Fig. \ref{comet_chartDet_B_woc_byTool_p3_5} and Fig. \ref{comet_chartDet_C_woc_byTool_p3_5} show the 
impact of compression; they also highlight the importance of incorporating compression directly in the tracing
tool. 
%
On average, \parlotnc slows down the application execution almost \textbf{2x} more than \parlota. 
%
The average overhead 
across Table \ref{comet_wo_det_All_all_B_p3.5} for \parlota is \textbf{3.42}.
%
The  corresponding figure for \parlotnc is \textbf{6.62}. 
%
The numbers of \parlotm and input C  follow the same pattern. For example, \parlotnc slows down the application execution almost \textbf{1.66x} more than \parlotm.
%

Clearly, compression not only lowers the storage requirement but also the overhead. This is important and shows that the extra computation to perform the compression are more than amortized by the reduction in the amount of data that need to be showed out.



\begin{figure}[t]
\centering
\includegraphics[width=3.8in]{figs.comet.newMed/comet_BX2_Main_16_B_p3_5.png}
\caption{ Variability of \parlotm overhead on 16 nodes - Input: B}
\label{comet_BX2_Main_16_B_p3_5}
\end{figure}



\ignore{--

\subsection{Extenuating Factors}

\begin{itemize}
\item \hl{We have enough confidence to scale up our experiments up to 4096 cores or even higher. However, Comet and other supercomputers that we had access to did not allow us larger runs}
\item \hl{ Our experiments were highly limited by the number of available SUs (service units) we had on multiple supercomputers. Running large-scale jobs for testing ParLOT with various parameters led us to running out of SUs several times throughout the experiments and we had to apply again for more allocations.}
\item \hl{ Finding HPC Scalable benchmarks was a challenge too. Other than NAS Parallel Benchmark (NPB), we had tried AMG2016, pKIFMM, HPGMG and several other well-known MPI applications but for different reasons such as library incompatibility or crashes/halts on some specific configurations, we were not able to run our general experiments on those benchmarks. That is why we stick NPB benchmark.}
\item \hl{Most of the HPC tracing tools are developed for performance analysis. To the best of our knowledge, Callgrind has the most similarity to our tool which collects function-call graph using a DBI framework. We have examined the tracing feature of tools such as Score-p and TAU but the comparison would be non-sense since the mechanism of collecting data and type of data that they collect are completely different.   
}
\end{itemize} 
}