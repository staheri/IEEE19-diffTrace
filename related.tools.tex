Instrumenting, profiling and tracing large-scale applications have become more popular to researchers and companies \cite{ddt} due to high demand of HPC users. 
Dyninst\cite{dyninst} is a dynamic instrumentation API which gives developers the ability to measure the performance \cite{openss}\cite{tau} and develop correctness debuggers \cite{stat}. It instruments the binary without any need of recompilation and also gives the developer the ability to attach instrumentation to a running process. VampirTrace\cite{vampirt} also uses Dyninst API to provide a library for collecting logs from program execution. 

The idea of analyzing execution traces for debugging purposes have been used in STAT\cite{stat} where it groups the processes with similar function-call stack and trying to find abnormal behavior like divergence in the function call-graph by delta debugging. The idea of tracing for debugging purposes have been used in other tools but first of all the overhead they add to the target application is high and are not straight-forward for HPC users who might not be a developer. They either need static instrumentation by inserting code-snippets and macros, or/and recompilation of the source code. 
Valgrind\cite{valgrind} is shadow value DBI framework (explained in the background section) that maps and records every register and memory value. It gives developers the capability of instrumenting system calls and instructions. Many error detectors such as \textit{Memcheck} have been built on top of Valgrind. \callgrind \cite{callgrind} is a profiling tool  on Valgrind platform that records the call history among functions in a program's run as a call-graph by measuring the number of instructions executed and their relationship to source lines. 
Intermediate generated traces by \callgrind are some numbered files contain pure ascii text. \callgrind enumerate the name of files and function calls and also, stores those numerical values as relative to previous numbers. These are the only data compression options available on \callgrind which is enabled by default. Each \callgrind trace file contains a sequence of function names (or their code) and a few other numbers for each function showing the that function relationships with other functions (caller-callee). There is a tool \textit{callgrind\_ annotate} which displays different reports from the generated traces. From the generated traces by \callgrind in my experiments, the richest report that \textit{callgrind\_ annotate} can produce is the tree of function calls with caller-callee relationship and cost of each function. Cost of each function is the number of Instruction Read which is collected during tracing by reading hardware counters. Cache simulation and branch prediction information also can be enabled to be collected and then \textit{callgrind\_ annotate} can produce different reports for cache and branch prediction. By default, cache simulation and branch prediction (which are originally from another tool Cachegrind) are disabled by \callgrind.

%and  \cite{ipm} \cite{tau} \cite{scorep} \cite{vtune}

The idea of compressing large-scale  traces have been used in \cite{eventflowgraph} for compressing performance traces and in ScalaTrace\cite{scalatrace} where it uses reptetive nature of timestep simulation in parallel scientific applications to compress traces\cite{freitag}. Only small fraction of compression is happening on-fly and the focus is on reducing inter-node communication. 