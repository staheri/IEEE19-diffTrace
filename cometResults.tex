%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Slowdown vs Callgrind
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\input{tabs.comet/comet_sd_pMpAcg_BC_int_p3.5.tex}


\input{tabs.comet/comet_sd_pMpAcg_BC_itn_p3.5.tex}

\subsection{ParLOT vs. Callgrind}

\begin{itemize}
\item \textbf{Slowdown}: Table \ref{comet_sd_pMpAcg_BC_int_p3.5} shows the slowdown of \parlot (pinMain and pinAll) and \callgrind. It shows slowdowns of same configuration (number of nodes/cores and size of input) next to each other so that we can look into it in more detail.
Table \ref{comet_sd_pMpAcg_BC_itn_p3.5} contains same exact numbers but grouped differently. In big picture, as average, by looking at bold numbers of table \ref{comet_sd_pMpAcg_BC_itn_p3.5}, experiments well shows that \parlot has better performance on larger input sizes which means longer runs. But for \callgrind it is opposite. For input size B, the average of geomeans of slowdowns is 3.82 and for input C it is 4.88. The key reason of this better performance is more repetition of target data to collect (which is function calls) on larger input sizes, I believe. Even when \parlot gathers system library function calls (pinAll), it has better performance than \callgrind. Figures \ref{comet_chartAvg_sd_B_p3_5} and \ref{comet_chartAvg_sd_C_p3_5} visualizes table \ref{comet_sd_pMpAcg_BC_itn_p3.5} numbers.
\item \textbf{Bandwidth}: Table \ref{comet_bw_pMpAcg_BC_itn_p3.5} shows the required bandwidth for each tool. In addition to big gap between average slowdown of \parlot(main) and \callgrind, \parlot(main) also beats \callgrind in required bandwidth, especially for smaller inputs.(adapted from Related Tools section) Valgrind is shadow value DBI framework (explained in the background section) that maps and records every register and memory value. It gives developers the capability of instrumenting system calls and instructions. Many error detectors such as \textit{Memcheck} have been built on top of Valgrind. \callgrind is a profiling tool  on Valgrind platform that records the call history among functions in a program's run as a call-graph by measuring the number of instructions executed and their relationship to source lines. 
Intermediate generated traces by \callgrind are some numbered files contain pure ascii text. \callgrind enumerate the name of files and function calls and also, stores those numerical values as relative to previous numbers. These are the only data compression options available on \callgrind which is enabled by default. Each \callgrind trace file contains a sequence of function names (or their code) and a few other numbers for each function showing the that function relationships with other functions (caller-callee). There is a tool \textit{callgrind\_ annotate} which displays different reports from the generated traces. From the generated traces by \callgrind in my experiments, the richest report that \textit{callgrind\_ annotate} can produce is the tree of function calls with caller-callee relationship and cost of each function. Cost of each function is the number of Instruction Read which is collected during tracing by reading hardware counters. Cache simulation and branch prediction information also can be enabled to be collected and then \textit{callgrind\_ annotate} can produce different reports for cache and branch prediction. By default, cache simulation and branch prediction (which are originally from another tool Cachegrind) are disabled by \callgrind.
According to table \ref{comet_cr_pMpA_BC_itn_p3.5}, for example for \parlot(all) where the average compression ratio for input C is 644.38, and the correspondent required bandwidth which is 56.38, it shows that \parlot can collect almost 36 MB worth of data per core per second where it only needs 56.38 KB/S bandwidth.]
\end{itemize}

\begin{figure}[!t]
\centering
\includegraphics[width=3.9in]{figs.comet/comet_chartAvg_sd_B_p3_5.png}
\caption{ Input: \textbf{B} - Slowdown of \parlot(main,all) and \callgrind. Each bar is the average slowdown of each tool on each application for 1, 4 and 16 nodes (16, 64 and 256 cores). Last group of bars is GeoMean (from bold numbers in table \ref{comet_sd_pMpAcg_BC_itn_p3.5}). 
}
\label{comet_chartAvg_sd_B_p3_5}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=3.9in]{figs.comet/comet_chartAvg_sd_C_p3_5.png}
\caption{ Input: \textbf{C} - Slowdown of \parlot(main,all) and \callgrind. Each bar is the average slowdown of each tool on each application for 1, 4 and 16 nodes (16, 64 and 256 cores). Last group of bars is GeoMean (from bold numbers in table \ref{comet_sd_pMpAcg_BC_itn_p3.5}). 
}
\label{comet_chartAvg_sd_C_p3_5}
\end{figure}




\subsection{\parlot Inner Analysis}
	\begin{itemize}
	\item \textbf{Compression Ratio}: Table \ref{comet_cr_pMpA_BC_itn_p3.5} shows the compression ratios for all configs and inputs. On average, \parlot can store up to more than 1700 MB of collected data in just 1 MB trace files. Compression ratios are higher for larger input sizes (reason: repetition of function calls). Also \parlot has better compression performance when it only collects function calls from main application image.
	\item \textbf{Pure \pin Overhead}: Tables \ref{comet_wo_det_Main_all_B_p3.5}, \ref{comet_wo_det_All_all_B_p3.5}, \ref{comet_wo_det_Main_all_C_p3.5} and  \ref{comet_wo_det_All_all_C_p3.5} show average overhead added to each application by different variations of \parlot. Reason of these experiments is to show the \textbf{impact of our data compression approach} and \textbf{pure overhead added by \pin}. \textbf{npin} is just the slowdown caused by initializing \pin's routines on top of the target application without doing anything else (no instrumentation, tracing, compression). In \textbf{wpin}, all collected data would be stored as is to the disk (tracing without compression) (fig \ref{overviewAll}). Last row of tables shows geometric mean of each of its above values showing how much each phase of ParLOT slows down the native execution. In general, we all expect that the slowdowns of $npin < ParLOT < wpin $. But majority of numbers are not like that. Highlighted cells in tables are the ones which do not follow the above order. Number of highlighted cells for \parlot(main) is larger than \parlot(all). Figures \ref{comet_chartAvg_serr_B_p3_5}, \ref{comet_chartAvg_serr_C_p3_5}, \ref{comet_chartAvg_var_B_p3_5} and  \ref{comet_chartAvg_var_C_p3_5} which are the standard errors and variances of runtimes, explain the reason of these highlighted cells. Variability of runtimes for \parlot(main) is more than other ones (native run, \parlot(all) and \callgrind). Maybe if I put the maximum or average of runtimes in the tables, the number of highlighted cells decreases.\\
	Figures \ref{comet_chartDet_B_wc_byTool_p3_5}, \ref{comet_chartDet_C_wc_byTool_p3_5}, \ref{comet_chartDet_B_woc_byTool_p3_5} and \ref{comet_chartDet_C_woc_byTool_p3_5} clearly show the performance of \parlot and impact of \parlot 's compression mechanism. (more explanations in chart caption of figure \ref{comet_chartDet_B_wc_byTool_p3_5} and \ref{comet_chartDet_B_woc_byTool_p3_5})
	\item \textbf{Impact of Compression} - Figures \ref{comet_chartDet_B_wc_byTool_p3_5}, \ref{comet_chartDet_C_wc_byTool_p3_5}, \ref{comet_chartDet_B_woc_byTool_p3_5} and \ref{comet_chartDet_C_woc_byTool_p3_5}
	\item \textbf{Variability} - Figures \ref{comet_chartAvg_serr_B_p3_5}, \ref{comet_chartAvg_serr_C_p3_5}, \ref{comet_chartAvg_var_B_p3_5} and  \ref{comet_chartAvg_var_C_p3_5}.
	\end{itemize}


\begin{figure}[!t]
\centering
\includegraphics[width=2in]{overview-all.png}
\caption{ Overview of \parlot}
\label{overviewAll}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bandwidth
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
% Compression Ratio
%%%%%%%%%%%%%%%%%%%%%%%



\input{tabs.comet/comet_bw_pMpAcg_BC_itn_p3.5.tex}

\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{figs.comet/comet_chartAvg_bw_B_p3_5.png}
\caption{ Input: \textbf{B} - Required Bandwidth per core (KB/s)
}
\label{comet_chartAvg_bw_B_p3_5}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{figs.comet/comet_chartAvg_bw_C_p3_5.png}
\caption{ Input: \textbf{C}  - Required Bandwidth per core (KB/s)
}
\label{comet_chartAvg_bw_C_p3_5}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%
% Detail runtimes
%%%%%%%%%%%%%%%%%%%%%%%

\input{tabs.comet/comet_cr_pMpA_BC_itn_p3.5.tex}


\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{figs.comet/comet_chartAvg_cr_C_p3_5.png}
\caption{ Input: \textbf{C}  - Compression Ratio
}
\label{comet_chartAvg_cr_C_p3_5}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{figs.comet/comet_chartAvg_cr_B_p3_5.png}
\caption{ Input: \textbf{B}  - Compression Ratio
}
\label{comet_chartAvg_cr_B_p3_5}
\end{figure}



\input{tabs.comet/comet_wo_det_Main_all_B_p3.5.tex}

\input{tabs.comet/comet_wo_det_All_all_B_p3.5.tex}


%\input{tabs.comet/comet_wo_det_Main_all_C_p3.5.tex}

%\input{tabs.comet/comet_wo_det_All_all_C_p3.5.tex}



\begin{figure}[!t]
\centering
\includegraphics[width=4in]{figs.comet/comet_chartDet_B_wc_byTool_p3_5.png}
\caption{ Input: \textbf{B} - This figure and figure \ref{comet_chartDet_C_wc_byTool_p3_5} shows how much of the overhead of \parlot is caused by \pin and its initialization and how much by that section of \parlot that collects traces and compress them. It seems that overhead added by pure \pin does not scale well and increases with growing number of cores.
}
\label{comet_chartDet_B_wc_byTool_p3_5}
\end{figure}


%Figure c wc
\begin{figure}[!t]
\centering
\includegraphics[width=4in]{figs.comet/comet_chartDet_C_wc_byTool_p3_5.png}
\caption{ Input: \textbf{C}
}
\label{comet_chartDet_C_wc_byTool_p3_5}
\end{figure}





\begin{figure}[!t]
\centering
\includegraphics[width=4in]{figs.comet/comet_chartDet_B_woc_byTool_p3_5.png}
\caption{ Input: \textbf{B}- This figure and figure \ref{comet_chartDet_C_woc_byTool_p3_5} shows the impact of \parlot 's data compression. By looking at and comparing green bars of figure \ref{comet_chartDet_B_wc_byTool_p3_5} and \ref{comet_chartDet_B_woc_byTool_p3_5}, clearly it is obvious that how much compressing data improves the performance.
}
\label{comet_chartDet_B_woc_byTool_p3_5}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=4in]{figs.comet/comet_chartDet_C_woc_byTool_p3_5.png}
\caption{ Input: \textbf{C}
}
\label{comet_chartDet_C_woc_byTool_p3_5}
\end{figure}








\begin{figure}[!t]
\centering
\includegraphics[width=4in]{figs.comet/comet_chartAvg_serr_B_p3_5.png}
\caption{ Input: \textbf{B}  - Standard Error of 3 Runtimes
}
\label{comet_chartAvg_serr_B_p3_5}
\end{figure}




\begin{figure}[!t]
\centering
\includegraphics[width=4in]{figs.comet/comet_chartAvg_serr_C_p3_5.png}
\caption{ Input: \textbf{C}  - Standard Error of 3 Runtimes
}
\label{comet_chartAvg_serr_C_p3_5}
\end{figure}





\begin{figure}[!t]
\centering
\includegraphics[width=4in]{figs.comet/comet_chartAvg_var_B_p3_5.png}
\caption{ Input: \textbf{B}  - Variance of 3 Runtimes
}
\label{comet_chartAvg_var_B_p3_5}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=4in]{figs.comet/comet_chartAvg_var_C_p3_5.png}
\caption{ Input: \textbf{C}  - Variance of 3 Runtimes
}
\label{comet_chartAvg_var_C_p3_5}
\end{figure}
