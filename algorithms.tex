
\subsection{NLR}
\label{subsec:algo-nlr}
Nested Loop Recognition (NLR) algorithm \cite{Ketterlin-nlr} is originally designed for prediction and compression of data access traces (memory addresses) through detecting the ``linear progression function'' in the sequence of addresses. Two main operations of NLR algorithm are 1) recognizing the start of a loop and forming its initial syntactic structure and 2) recognizing if what follows a loop is just another iteration and extend the upper bound. These operations are performed incrementally and recursively to recognize loops at different depths (linear function of loop index $i$ and depth $d$) until the input sequence is completely consumed and no more loop is detected.

Previous to NLR, Kobayashi \cite{kobayashi-84} proposed a similar bottom-up strategy to build loop nests from a trace (sequence of string instructions) where each recognized loop is replaced with a new symbol, which is remembered so that identical loops at different locations can be identified. This process is restarted once the whole trace has been analyzed for depth-2 loops, and so on until some replacement is performed.

Inspired by Kobayashi approach, we have re-implemented the NLR algorithm (DiffTrace-NLR) for detecting repetitive patterns in a sequence of function calls (trace) and summarizing the trace to NLR representation. DiffTrace-NLR works by incrementally pushing trace entries (function IDs) into a stack of \textit{elements} (i.e., function IDs and already detected loop structures). Whenever an element is pushed to the stack, the upper elements of the stack are recursively examined for potential loop detection or loop extensions (Procedure \ref{proc:NLR}). 



\begin{algorithm}[]
 \DontPrintSemicolon
 \SetKwFunction{KwReduce}{Reduce}
% \SetKwInOut{Input}{Input} \SetKwInOut{Output}{Output}\SetKwInOut{Local}{Local}
  %\SetKw{KwEach}{each}
 %\Input{Stack of elements $S$, $S[1]$ is top}
 %\Output{$NLR(T)$}
 \KwReduce{$S$}:{\\
 \Indp
 	\For{$ i:1$ ... $3K$}{
 		$b$ = $i/3$\;
 		\If{Top 3 $b$-long elements of $S$ are \textit{isomorphic}}{
 			pop $i$ elements from $S$\;
 			$LB=S[b:1]$
 			$LC=3$\;
 			$LS=(LB,LC)$\;
 			push $LS$ to $S$\;
 			add $LB$ to Loop Table\;
 			\KwReduce{$S$}\;
 		}
 		\If{ $S[i]$ is a loop ($LS$) and $S[i-1:1]$ isomorphic to its loop body$LB$}{
 			$LC=LC+1$\;
 			pop $i-1$ elements from $S$\;
 			\KwReduce{$S$}\;
 		}
 	}
 }

 \caption{\texttt{Reduce} Procedure Adapted From the NLR algorithm }
 \label{proc:NLR}
\end{algorithm}

Each loop structure \textbf{LS} is a tuple of (loop body \textbf{LB}, loop count \textbf{LC}) where LB is a sequence of \textit{elements} and LC is an integer showing the frequency of consecutive occurrence of LB. To avoid coincidental regularity, the algorithm needs at least three consecutive terms to form a LS. The \texttt{Reduce} procedure checks to see if any three consecutive sub-sequences on top of the stack are \textit{isomorphic}. In our context, subsequences are isomorphic if they have equal lengths and all correspoding elements of each subsequence are identical. Then the three subsequences are popped from the stack and the LS=(LB,3) is pushed to stack (LB is either of subsequences). We store all distinct LBs into a hash-table, assigning each a unique ID which can be applied as heuristic to detect loops in other traces and compact representation of traces (NLR). 
 The maximum length of subsequences to examin is decided by a fixed priori $K$.  The complexity of NLR algorithm is $\Theta(K^2N)$ where $N$ is the size of the input. 
%



\subsection{Concept Lattice Construction}
\label{subsec:algo-cl}


There are algorithms that can extract concepts and their partial order from a formal context as a batch or incrementally. For large scale executions 

Other kinds of similarity indexes can be extracted from concept lattices \cite{Alqadah2011}, in addition to many applications of concept lattices  properties have been exploited in many computer science fields have been e different similarity metrics can be derived from the

\subsection{Hierarchical Clustering, Construction and Comparison}
 \label{subsec:algo-bscore}
JSMs and Linkage functions from Scipy 
B-score to compare clusterings