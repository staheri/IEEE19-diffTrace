
\subsection{NLR}
\label{subsec:algo-nlr}
Nested Loop Recognition (NLR) algorithm \cite{Ketterlin-nlr} is originally designed for prediction and compression of data access traces (memory addresses) through detecting the ``linear progression function'' in the sequence of addresses. Two main operations of NLR algorithm are 1) recognizing the start of a loop and forming its initial syntactic structure and 2) recognizing if what follows a loop is just another iteration and extend the upper bound. These operations are performed incrementally and recursively to recognize loops at different depths (linear function of loop index $i$ and depth $d$) until the input sequence is completely consumed and no more loop is detected.

Previous to NLR, Kobayashi \cite{kobayashi-84} proposed a similar bottom-up strategy to build loop nests from a trace (sequence of string instructions) where each recognized loop is replaced with a new symbol, which is remembered so that identical loops at different locations can be identified. This process is restarted once the whole trace has been analyzed for depth-2 loops, and so on until some replacement is performed.

Inspired by Kobayashi approach, we have re-implemented the NLR algorithm (DiffTrace-NLR) for detecting repetitive patterns in a sequence of function calls (trace) and summarizing the trace to NLR representation. DiffTrace-NLR works by incrementally pushing trace entries (function IDs) into a stack of \textit{elements} (i.e., function IDs and already detected loop structures). Whenever an element is pushed to the stack, the upper elements of the stack are recursively examined for potential loop detection or loop extensions (Procedure \ref{proc:NLR}). 



\begin{algorithm}[]
 \DontPrintSemicolon
 \SetKwFunction{KwReduce}{Reduce}
% \SetKwInOut{Input}{Input} \SetKwInOut{Output}{Output}\SetKwInOut{Local}{Local}
  %\SetKw{KwEach}{each}
 %\Input{Stack of elements $S$, $S[1]$ is top}
 %\Output{$NLR(T)$}
 \KwReduce{$S$}:{\\
 \Indp
 	\For{$ i:1$ ... $3K$}{
 		$b$ = $i/3$\;
 		\If{Top 3 $b$-long elements of $S$ are \textit{isomorphic}}{
 			pop $i$ elements from $S$\;
 			$LB=S[b:1]$
 			$LC=3$\;
 			$LS=(LB,LC)$\;
 			push $LS$ to $S$\;
 			add $LB$ to Loop Table\;
 			\KwReduce{$S$}\;
 		}
 		\If{ $S[i]$ is a loop ($LS$) and $S[i-1:1]$ isomorphic to its loop body$LB$}{
 			$LC=LC+1$\;
 			pop $i-1$ elements from $S$\;
 			\KwReduce{$S$}\;
 		}
 	}
 }

 \caption{\texttt{Reduce} Procedure Adapted From the NLR algorithm }
 \label{proc:NLR}
\end{algorithm}

Each loop structure \textbf{LS} is a tuple of (loop body \textbf{LB}, loop count \textbf{LC}) where LB is a sequence of \textit{elements} and LC is an integer showing the frequency of consecutive occurrence of LB. To avoid coincidental regularity, the algorithm needs at least three consecutive terms to form a LS. The \texttt{Reduce} procedure checks to see if any three consecutive sub-sequences on top of the stack are \textit{isomorphic}. In our context, subsequences are isomorphic if they have equal lengths and all correspoding elements of each subsequence are identical. Then the three subsequences are popped from the stack and the LS=(LB,3) is pushed to stack (LB is either of subsequences). We store all distinct LBs into a hash-table, assigning each a unique ID which can be applied as heuristic to detect loops in other traces and compact representation of traces (NLR). 
 The maximum length of subsequences to examin is decided by a fixed priori $K$.  The complexity of NLR algorithm is $\Theta(K^2N)$ where $N$ is the size of the input. 
%



\subsection{Concept Lattice Construction}
\label{subsec:algo-cl}

There exist algorithms that extract concepts and their partial order from a formal context, each has their own pros and cons.
%
Given the density/sparsness of the formal context, the efficiency of algorithms vary \cite{clgenperform}.
%
The basic Ganter's \textit{Next Closure} algorithm \cite{clbook} construct the lattice from the \textit{batch} of context and requires the whole context to be present in the main memory.
%
For large scale HPC traces, such an approach is inefficient.
%
We have implemented Godin's \textit{incremental} algorithm \cite{clconst} to extract concepts from traces and inject them to an initially empty  lattice.
%
Every time a new concept is added to the lattice, an \textit{update} procedure minimally modifies/adds/deletes edges and nodes to the lattice. 
%
This update procedure is guaranteed to preserve the validity of the concept lattice properties.
%
The complexity of Godin's algorithm is $O(2^{2K}|G|)$ where $K$ is an upper bound for number of attributes (e.g., distinct function calls in the whole execution) and $|G|$ is the number of objects (e.g., number of traces).

%In DiffTrace, the upper bound for number of attribute is usually small, since traces are pre-processed and summarized in form of NLRs. Thus Godin's algorithm would perform near



\subsection{Hierarchical Clustering, Construction and Comparison}
 \label{subsec:algo-bscore}
We have used 

cite scipy 
used scipy library to construct hierarchical clustering (linkage) (table) for generating the ranking table. 