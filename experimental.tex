\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.31\textwidth}
        \centering
\includegraphics[width=\textwidth]{figs/diffNLR/ompBug-6-4-x0.pdf}
\caption{diffNLR(6.4)}
\label{diffNLR-6-4}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.31\textwidth}
       \centering
\includegraphics[width=\textwidth]{figs/diffNLR/mpiBug-all-nn-x0.pdf}
\caption{diffNLR(0)}
\label{diffNLR-0}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
\includegraphics[width=0.9\textwidth]{figs/diffNLR/mpiBug2-0-nn-x0.pdf}
\caption{diffNLR(5)}
\label{diffNLR-5}
     \end{subfigure}
        \caption{Three simple graphs}
        \label{fig:three graphs}
\end{figure*}

ILCS is a scalable framework for running iterative local searches on HPC platforms~\cite{ilcs}.
%
Providing serial CPU and/or single-GPU code, ILCS executes this code in parallel between compute nodes (MPI) and within them (OpenMP and CUDA).
%

To evaluate DiffTrace, we manually injected MPI-level and OMP-level bugs into the Traveling Salesman Problem (TSP) running on ILCS (Listing~\ref{lst:ilcs}).
%
The injected bugs simulate real HPC bugs such as deadlocks.
%
%These bugs are close to common mistakes that HPC developers usually make during developing HPC codes.
%
Moreover, we inserted ``hidden'' faults that do not crash the program such as violations of critical sections and semantic bugs. 
%
%The injected bugs are planted in a way that might get triggered in only one or more threads (master and worker threads, one thread, every other thread, all threads except one, all threads). 
%
The goal is to see how effectively DiffTrace can analyze the resulting traces and how close it can get to the root cause of the fault.
%
\input{tabs/ilcsPseudoCode-compact.tex}

%
We collected ParLoT (main image) traces from the execution of ILCS-TSP with 8 MPI processes and 4 OpenMP threads per process on the XSEDE-PSC Bridges supercomputer whose compute nodes have 128 GB of main memory and contain 2 Intel Haswell (E5-2695 v3) CPUs with 14 cores each running at 2.3 - 3.3 GHz.
%
Note that we did not provide any GPU code to ILCS.
%
The collected traces (faulty and normal) are fed to DiffTrace. We enabled the MPI, OpenMP, and custom (ILCS-TSP user code) filters and set the NLR constant K to 10 for all experiments.
%
We present the results in form of ranking tables that show which traces (processes and threads) DiffTrace considers ``suspicious''. Furthermore, we show diffNLRs for selected traces.

\subsection{ILCS-TSP Workflow}

\hl{2-3 sentences about how ILCS finds local champions in TSP problem}
There are two types of threads in ILCS: a \textit{master} thread per node (MPI process) and a set of \textit{worker} threads per compute node (OpenMP threads)
%
Master threads of compute nodes are in charge of handling local working threads and communicating with master threads on other nodes.
%
For each detected CPU core, the master thread forks worker OpenMP threads.
%
Each worker thread continually calls
\texttt{CPU\_Exec()} to evaluate a range of seeds and record the results (lines 14-20).
%
Once the worker threads are running, the master thread's primary job is to scan the results of the workers to find the best solution computed so far (i.e., the local champion). This information is then globally reduced to determine the current system-wide champion (lines 22-32).
%
Since scanning the entire seed range in a reasonable amount of time is not feasible, ILCS terminates the search when the quality has not improved over a certain period (lines 33-34).

\subsection{OpenMP Bug: Unprotected Memory Access}





The memory accesses of \texttt{memcpy} in line 20 and 30 are protected by OpenMP critical section.
%
If under some scenario, this shared memory location becomes unprotected, a race condition might happen and invalidate the ILCS final output.
%
We have simulated such a scenario and modified the ILCS source code so that the control flow of the program skip the critical section in some specific OpenMP threads.
%
In one case where we inject this bug to the worker thread 4 of process 6, DiffTrace generated Table~\ref{tab:mc1-mc-6-4} as top suspicious traces for further analysis.
%
Each table entry contains the parameter that leads to the last two column suggestions.
%
For example, filter ``11.mem.ompcit.cust.0K10'' briefly means that all returns and .plt calls have been removed from traces of both faulty and normal executions, and only memory-related functions, OpenMP critical section functions and custom function ``CPU\_Exec'' are kept in traces.
%
The K10 at the end of filter means that all filtered traces are converted to their equivalent NLR with $K$=10.
%
The rest of the parameters have been explained in previous sections.
%
\hl{I will remove two unnecessary columns from the tables (threshold and linkage function) to save space and add 2-3 sentences explaining what was them}

The bold numbers in the last column are suggesting trace \textbf{6.4} (process 6, thread 4) as the trace that changed the most after we planted the bug.
%
diffNLR(6.4) in Figure~\ref{diffNLR-6-4} clearly shows that the normal execution of ILCS (blue blocks) protects the \texttt{memcpy} while the buggy execution does not. In this figure, L0 is \texttt{CPU\_Exec}, which has been executed several times in both versions but never reaches the optimal solution until the end.
%


%\begin{figure}[]
%\centering
%\includegraphics[width=0.3\textwidth]{figs/diffNLR/ompBug-6-4.pdf}
%\caption{OpenMP Bug: diffNLR(6.4)}
%\label{diffNLR-6-4}
%\end{figure}



\subsection{MPI Bug: Deadlock Caused by Fault in Collectives}
By forcing only one of the processes (process 2) to invoke MPI\_Allreduce (line 24) with a wrong size, we have simulated a \textit{real deadlock}. 
%
Table~\ref{tab:ar1-ws-all-nn} shows that almost all processes are suspicious.
%
It turned out that ParLOT did not happen to capture function calls from all processes since the bug happens too early in the code. Thus except for process 1 and 4, all other traces are empty.
%
By looking at the diffNLR(1) (Figure~\ref{diffNLR-0}), we can see that both normal and the buggy trace of process $1$ are identical until an invocation of MPI\_Allreduce(). After that, normal trace hits the end of the program and terminates while the buggy process is waiting for the return from the actual point of fault (process 2) and never ends (i.e., deadlocks). 
%
diffNLRs of other processes look the same.
%

%\input{tabs/ar1-ws-all-nn.tex}

%\begin{figure}[]
%\centering
%\includegraphics[width=0.3\textwidth]{figs/diffNLR/mpiBug-all-nn.pdf}
%\caption{diffNLR(0)}
%\label{diffNLR-0}
%\end{figure}
%

\input{tabs/mc1-mc-6-4.tex}

\input{tabs/ar1-ws-all-nn.tex}

\subsection{MPI Bug: Wrong Collective Operation}
By changing the operation MPI\_MIN to MPI\_MAX in the input arguments of MPI\_Allreduce(), we have changed the semantics of ILCS. 
%
The execution of this variation terminated well, but the results might be corrupted.
%

The MPI\_Allreduce() in line 24 of Listing~\ref{lst:ilcs} broadcasts the best-calculated answer among all processes.
%
However, by the change that we made to ILCS, now the ``worst'' answer is getting stored.
%
We injected the bug only to process 0.
%
Among all suggested suspicious processes (Table~\ref{tab:ar1-wo-0-nn}), only process 5 (bold numbers) are making sense since their filters are more relevant to the aspect that we are interested (MPI-level activities) to study deeper.
%
Our observation from diffNLR(5) (Figure~\ref{diffNLR-5}) is that process 5, in comparison with its corresponding normal process, involves more in updating and broadcasting the champion among all traces.
%
Similar to the deadlock bug, this is another instance of  ``bug manifestation'' detection by DiffTrace.

\input{tabs/ar1-wo-0-nn.tex}

%\begin{figure}[]
%\centering
%\includegraphics[width=0.3\textwidth]{figs/diffNLR/mpiBug2-0-nn.pdf}
%\caption{diffNLR(5)}
%\label{diffNLR-5}
%\end{figure}



